{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Bv7Y2o9EA4LR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on CoLab\n"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Running on CoLab')\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')   \n",
    "\n",
    "    import os\n",
    "    os.chdir('/content/drive/My Drive/Deep_learning_project/')\n",
    "\n",
    "else:\n",
    "    print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ujl8ejBPyV4l"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import datetime\n",
    "import itertools\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9f-m_NIcyV41"
   },
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gGgQwbUJyV49"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('GM_preparedData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "98uOUHFryV5C"
   },
   "outputs": [],
   "source": [
    "def format_tripStart(x, **kwargs):\n",
    "    if hasattr(x, 'hour'):\n",
    "        hour = x.hour * 60 * 60\n",
    "        minute = x.minute * 60\n",
    "        time_seconds = hour + minute\n",
    "    else:\n",
    "        x = datetime.timedelta(hours=x)\n",
    "        time_seconds = x.seconds\n",
    "\n",
    "    encoded_time = kwargs['encoder'](2 * np.pi * time_seconds / (24 * 60 * 60))\n",
    "    \n",
    "    return encoded_time\n",
    "\n",
    "\n",
    "def format_time_split_hours(x):\n",
    "    \n",
    "    return int(x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5kWaKF-3yV5G"
   },
   "outputs": [],
   "source": [
    "# Define rectangle where \"valid data belongs\":\n",
    "\n",
    "upper_right = (55.880120, 12.707644)\n",
    "lower_left = (55.548626, 12.061736)\n",
    "\n",
    "# Clean data\n",
    "df_clean = df.loc[(df['startPositionLat'] >= lower_left[0]) & (df['startPositionLat']<=upper_right[0])]\n",
    "df_clean = df_clean.loc[(df_clean['startPositionLng'] >= lower_left[1]) & (df_clean['startPositionLng']<=upper_right[1])]\n",
    "df_clean = df_clean.dropna(axis=0, subset=['startPositionLat', 'startPositionLng', 'tripStart'])\n",
    "\n",
    "df_clean['tripStart'] = pd.to_datetime(df_clean['tripStart'], format='%d%b%y:%H:%M:%S')\n",
    "#df_clean['timeOfDay'] = df_clean['tripStart'].apply(format_tripStart)\n",
    "df_clean['cos_timeOfDay'] = df_clean['tripStart'].apply(format_tripStart, encoder=np.cos)\n",
    "df_clean['sin_timeOfDay'] = df_clean['tripStart'].apply(format_tripStart, encoder=np.sin)\n",
    "\n",
    "df_clean['timeHour'] = df_clean['tripStart'].apply(format_time_split_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BJug_gHoyV5M"
   },
   "outputs": [],
   "source": [
    "df_clean = df_clean[['startPositionLat', 'startPositionLng', 'sin_timeOfDay', 'cos_timeOfDay', 'timeHour']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Lld5N7REPZgI"
   },
   "outputs": [],
   "source": [
    "data_columns = ['startPositionLng', 'startPositionLat', 'sin_timeOfDay', 'cos_timeOfDay']\n",
    "data = df_clean[data_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-dZgHGueyV5a"
   },
   "outputs": [],
   "source": [
    "#Normalize data\n",
    "train, test = train_test_split(data.values, test_size=0.1, random_state=42)\n",
    "df_train = pd.DataFrame(train, columns=data.columns)\n",
    "df_test = pd.DataFrame(test, columns=data.columns)\n",
    "\n",
    "mu_lat = df_train['startPositionLat'].mean()\n",
    "sig_lat = df_train['startPositionLat'].std()\n",
    "\n",
    "mu_lng = df_train['startPositionLng'].mean()\n",
    "sig_lng = df_train['startPositionLng'].std()\n",
    "\n",
    "df_train['NormLng'] = df_train['startPositionLng'].apply(lambda x: (x-mu_lng)/sig_lng)\n",
    "df_train['NormLat'] = df_train['startPositionLat'].apply(lambda x: (x-mu_lat)/sig_lat)\n",
    "\n",
    "\n",
    "mu_cos = df_train['cos_timeOfDay'].mean()\n",
    "sig_cos = df_train['cos_timeOfDay'].std()\n",
    "\n",
    "mu_sin = df_train['sin_timeOfDay'].mean()\n",
    "sig_sin = df_train['sin_timeOfDay'].std()\n",
    "\n",
    "df_train['Normsin'] = df_train['sin_timeOfDay'].apply(lambda x: (x-mu_sin)/sig_sin)\n",
    "df_train['Normcos'] = df_train['cos_timeOfDay'].apply(lambda x: (x-mu_cos)/sig_cos)\n",
    "\n",
    "\n",
    "# Normalize validation set\n",
    "test_mu_lat = df_test['startPositionLat'].mean()\n",
    "test_sig_lat = df_test['startPositionLat'].std()\n",
    "\n",
    "test_mu_lng = df_test['startPositionLng'].mean()\n",
    "test_sig_lng = df_test['startPositionLng'].std()\n",
    "\n",
    "df_test['NormLng'] = df_test['startPositionLng'].apply(lambda x: (x-test_mu_lng)/test_sig_lng)\n",
    "df_test['NormLat'] = df_test['startPositionLat'].apply(lambda x: (x-test_mu_lat)/test_sig_lat)\n",
    "\n",
    "test_mu_cos = df_test['cos_timeOfDay'].mean()\n",
    "test_sig_cos = df_test['cos_timeOfDay'].std()\n",
    "\n",
    "test_mu_sin = df_test['sin_timeOfDay'].mean()\n",
    "test_sig_sin = df_test['sin_timeOfDay'].std()\n",
    "\n",
    "df_test['Normsin'] = df_test['sin_timeOfDay'].apply(lambda x: (x-test_mu_sin)/test_sig_sin)\n",
    "df_test['Normcos'] = df_test['cos_timeOfDay'].apply(lambda x: (x-test_mu_cos)/test_sig_cos)\n",
    "\n",
    "data_columns = ['NormLng', 'NormLat', 'Normsin', 'Normcos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uDI_11LfyV5b"
   },
   "outputs": [],
   "source": [
    "data_train = df_train[['NormLng', 'NormLat', 'Normsin', 'Normcos']]\n",
    "data_test = df_test[['NormLng', 'NormLat', 'Normsin', 'Normcos']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WH6W48LeyV5d"
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorConditioner_scaleFix(nn.Module):\n",
    "    \"\"\"\n",
    "    This PyTorch Module implements the neural network used to condition the\n",
    "    base distribution of a NF as in Eq.(13).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(PriorConditioner, self).__init__()\n",
    "\n",
    "        #initialize linear transformations\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.lin_input_to_hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lin_hidden_to_hidden = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.lin_hidden_to_loc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.lin_hidden_to_scale = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        #initialize non-linearities\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Given input x=[z_{t-1}, h_t], this method outputs mean and \n",
    "        std.dev of the diagonal gaussian base distribution of a NF.\n",
    "        \"\"\"\n",
    "        hidden = self.relu(self.lin_input_to_hidden(x))\n",
    "        hidden = self.relu(self.lin_hidden_to_hidden(hidden))\n",
    "        loc = self.lin_hidden_to_loc(hidden)\n",
    "        scale = torch.ones_like(loc)\n",
    "        return loc, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VANILLA TIME CONDITIONED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.zeros(4).to(device)\n",
    "sigma = torch.ones(4).to(device)\n",
    "base = Normal(mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RealNVP(in_features=4, prior=base, hidden_features=256, depth=12).to(device)\n",
    "optimizer = torch.optim.Adam([p for p in net.parameters() if p.requires_grad], lr=1e-4)\n",
    "\n",
    "pyro_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=25, verbose=True)\n",
    "\n",
    "# Arbitrary clipping value to suppress exploding gradients\n",
    "clipping_value = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(data_train.values).to(device)\n",
    "X_test = torch.Tensor(data_test.values).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/500 [00:21<2:55:04, 21.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 1.6095812320709229\n",
      "Train loss: -1.1750142817988398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/500 [00:41<2:54:14, 20.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7747608423233032\n",
      "Train loss: -0.8280409979783814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/500 [01:03<2:54:49, 21.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -1.9291881322860718\n",
      "Train loss: -1.5922084609202398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/500 [01:24<2:54:12, 21.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -1.0189944505691528\n",
      "Train loss: -2.4112359784145574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/500 [01:45<2:53:51, 21.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 4.799684524536133\n",
      "Train loss: -3.447558910656658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/500 [02:06<2:53:05, 21.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -3.218385696411133\n",
      "Train loss: -3.9612562047698785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 7/500 [02:29<2:57:23, 21.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -4.085292816162109\n",
      "Train loss: -4.156296970454576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/500 [02:51<2:58:55, 21.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -3.984652280807495\n",
      "Train loss: -3.4129000825392732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9/500 [03:12<2:57:09, 21.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -3.5731124877929688\n",
      "Train loss: -4.263806148845717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/500 [03:33<2:55:40, 21.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -0.790239691734314\n",
      "Train loss: -4.397553760093305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 11/500 [03:56<2:56:50, 21.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -2.293008804321289\n",
      "Train loss: -4.354164738164905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 12/500 [04:16<2:53:35, 21.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -3.6160433292388916\n",
      "Train loss: -4.51118994050658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 13/500 [04:37<2:51:09, 21.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -4.730881690979004\n",
      "Train loss: -4.622946634332492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 14/500 [04:58<2:52:14, 21.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -3.167951822280884\n",
      "Train loss: -4.6433595433505195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 15/500 [05:24<3:03:40, 22.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -0.7395848035812378\n",
      "Train loss: -4.670155524900674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 16/500 [05:49<3:08:06, 23.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: -3.414114475250244\n",
      "Train loss: -4.729708460441274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 16/500 [06:09<3:06:12, 23.08s/it]\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "validation_losses = []\n",
    "save_flag = True\n",
    "epochs = 500\n",
    "\n",
    "try:\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        loss_epoch = 0\n",
    "\n",
    "        # VALIDATION\n",
    "        net.eval()\n",
    "\n",
    "        validation_loss = -torch.mean(net.log_likelihood(X_test))\n",
    "        validation_losses.append(validation_loss.item())\n",
    "        \n",
    "        pyro_scheduler.step(validation_loss)\n",
    "\n",
    "        # TRAIN LOOP\n",
    "        net.train()\n",
    "        for batch_num, (X_batch) in enumerate(train_loader):\n",
    "            X_batch = X_batch[0]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = -torch.mean(net.log_likelihood(X_batch))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_epoch += loss.item()\n",
    "\n",
    "        loss_epoch = loss_epoch / (batch_num+1)\n",
    "        train_losses.append(loss_epoch)\n",
    "\n",
    "        if (epoch) % 1 == 0:\n",
    "            print(f'Validation loss: {validation_loss}')\n",
    "            print(f'Train loss: {loss_epoch}')\n",
    "\n",
    "        if (epoch) % 50 == 0:\n",
    "            torch.save(net.state_dict(), 'model_state_fix_prior_time_cond_updated_epoch_%d.pth' % (epoch+1))\n",
    "    \n",
    "    if save_flag:\n",
    "        loss_dict = {'validation': validation_losses, 'train': train_losses}\n",
    "        with open('loss_dict_time_cond_fix_prior.pkl', 'wb') as f:\n",
    "            pickle.dump(loss_dict, f)\n",
    "\n",
    "        torch.save(net.state_dict(), 'model_state_fix_prior_cond.pth')\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    if save_flag:\n",
    "        loss_dict = {'validation': validation_losses, 'train': train_losses}\n",
    "        with open('loss_dict_time_cond_fix_prior.pkl', 'wb') as f:\n",
    "            pickle.dump(loss_dict, f)\n",
    "\n",
    "        torch.save(net.state_dict(), 'model_state_fix_prior_time_cond.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Time Conditioned NF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
